{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere, we ask GPT-3.5 to clean our raw dataset for us. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here, we ask GPT-3.5 to clean our raw dataset for us. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv # pip install python-dotenv\n",
    "load_dotenv()   # Set API KEY values from .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset (sentences that have been extracted, with confidence labels)\n",
    "sentences_with_labels_raw = pd.read_csv('data/text_processing/sentences_with_ratings_04_20.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sentences, including unlabeled (to be used for context)\n",
    "all_sentences_raw = pd.read_csv('data/text_processing/all_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context for a labeled sentence.\n",
    "def get_context(filename, pg_num, sent_num, \n",
    "                n_sentences_before=5, n_sentences_after=2): # Context is more likely to be before, than after.\n",
    "    try:\n",
    "        # Find the row index of the entry\n",
    "\n",
    "        filtered_df = all_sentences_raw[all_sentences_raw['filenames'] == filename].reset_index(drop=True)\n",
    "        row_index = filtered_df[(filtered_df['page_num'] == pg_num) &\n",
    "                                (filtered_df['sent_num'] == sent_num)].index[0]\n",
    "        \n",
    "        # Get the indices of the rows before and after the target row\n",
    "        indices = list(range(max(0, row_index - n_sentences_before), min(row_index + n_sentences_after, len(filtered_df))))\n",
    "\n",
    "        # Concatenate the sentences\n",
    "        context = \" \".join(filtered_df.loc[indices, 'text'])\n",
    "        return context\n",
    "    except IndexError:\n",
    "        print(\"Entry not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"You are DataCleanerGPT, an assistant who rewrites input texts such that they are complete sentences, containing enough information for a human to evaluate the factualness of the statement. This includes replacing references to outside information with the referred-to information, adding external context of what is being discussed or measured so that a human can evaluate the truth of the statement, and removing extraneous qualifiers such as \"Therefore,\" or \"In conclusion.\" It also means removing artifacts of web scraping such as irrelevant characters/symbols, sentence fragments, or numbers in strange places in the sentence. You should also remove indications of confidence/likelihood from the sentence, and avoid them in your revision. Preserve the original meaning of the sentence (within the context given) as much as possible. Your response can only be ONE SENTENCE long. \n",
    "\n",
    "You will be given a sentence and context within which that sentence was found. You can respond in one of two ways: \n",
    "1. If the sentence requires rewriting and it is possible to do so, respond with the rewritten sentence.\n",
    "2. If the sentence requires rewriting but it is not possible to do so because there is not enough context provided, or the original sentence is not meaningful, respond with \"unrecoverable\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on index: 0\n",
      "working on index: 10\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "MODEL=\"gpt-3.5-turbo\"\n",
    "\n",
    "results = []\n",
    "# Iterate over each row in selected_sentences_df, construct the prompt, feed to GPT\n",
    "for index, row in sentences_with_labels_raw.iterrows():\n",
    "    if index % 10 == 0:\n",
    "        print(f\"working on index: {index}\")\n",
    "    if index % 100 == 0:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        model_str = MODEL.replace('.', 'p')\n",
    "        results_df.to_csv(f'data/text_processing/sentences_with_ratings_05_25_revised_{model_str}.csv', index=False)\n",
    "\n",
    "\n",
    "    # Get the filename, page_number, sentence_number, and sentence from the current row\n",
    "    filename = row['filenames']\n",
    "    page_number = row['page_num']\n",
    "    sentence_number = row['sent_num']\n",
    "    sentence = row['text']\n",
    "    conf_rating = row['confidence_rating']\n",
    "    \n",
    "    # Get the concatenated sentences as context for the current sentence\n",
    "    context = get_context(filename, page_number, sentence_number)\n",
    "    \n",
    "    input = f\"\"\"Sentence: {sentence}\n",
    "    Context: {context}\n",
    "    Response: \"\"\"\n",
    "\n",
    "    # print(\"input: \")\n",
    "    # print(input)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": input}\n",
    "        ]\n",
    "    )\n",
    "    rewrite = response.choices[0].message.content\n",
    "\n",
    "    # print()\n",
    "    # print()\n",
    "    # print(\"output\")\n",
    "    # print(rewrite)\n",
    "\n",
    "    # print(\"--------\")\n",
    "    results.append({\n",
    "        'filenames': filename,\n",
    "        'page_num': page_number,\n",
    "        'sent_num': sentence_number,\n",
    "        'text': sentence,\n",
    "        'gpt_revised_text': rewrite,\n",
    "        'confidence_rating': conf_rating,\n",
    "        'context': context\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "model_str = MODEL.replace('.', 'p')\n",
    "results_df.to_csv(f'data/text_processing/sentences_with_ratings_05_25_revised_{model_str}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "model_str = MODEL.replace('.', 'p')\n",
    "results_df.to_csv(f'data/text_processing/sentences_with_ratings_05_25_revised_{model_str}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
