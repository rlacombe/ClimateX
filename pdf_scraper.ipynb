{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import PyPDF2 as pdf\n",
    "# from dotenv import load_dotenv # pip install python-dotenv\n",
    "# load_dotenv()   # Set API KEY values from .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to the PDFs\n",
    "files = [\n",
    "    'data/raw/IPCC_AR6_WGI_FullReport.pdf',\n",
    "    'data/raw/IPCC_AR6_WGII_FullReport.pdf',\n",
    "    'data/raw/IPCC_AR6_WGIII_FullReport.pdf'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/Author': 'IPCC AR6 Working Group I', '/CreationDate': \"D:20220725145825+02'00'\", '/Creator': 'Adobe Acrobat Pro DC (32-bit) 22.1.20169', '/ModDate': \"D:20220726075121-04'00'\", '/Producer': 'Adobe Acrobat Pro DC (32-bit) 22.1.20169', '/Title': 'Climate Change 2021: The Physical Science Basis'}\n"
     ]
    }
   ],
   "source": [
    "# Iterate through files in list, extract raw text\\# takes ~1 min / 100 pages on my macbook.\n",
    "# NUM_PAGES = 1000\n",
    "\n",
    "\n",
    "documents = []\n",
    "for file in files: \n",
    "    document = {}\n",
    "\n",
    "    read_pdf = pdf.PdfReader(file)\n",
    "    print(read_pdf.metadata)\n",
    "    document['filepath'] = file\n",
    "    document['metadata'] = read_pdf.metadata\n",
    "    document['text_by_page'] = [page.extract_text() for page in read_pdf.pages]\n",
    "    documents.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing functions \n",
    "import re \n",
    "\n",
    "def normalize_whitespace(input):\n",
    "    # remove newlines, tabs, and double spaces, convert them all to single spaces\n",
    "    return \" \".join(input.split())\n",
    "\n",
    "def strip_paren_whitespace(input):\n",
    "    # Remove whitespace before close paren and after open paren\n",
    "    return re.sub('(\\s([?,.!\"]))|(?<=\\[|\\()(.*?)(?=\\)|\\])', lambda x: x.group().strip(), input)\n",
    "\n",
    "for document in documents:\n",
    "    document['text_by_page_processed'] = [strip_paren_whitespace(normalize_whitespace(page)) for page in document['text_by_page']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try splitting into sentences with nltk based on https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "for document in documents:\n",
    "    document['text_by_page_sentences'] = [sent_tokenize(page) for page in document['text_by_page_processed']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085\n"
     ]
    }
   ],
   "source": [
    "# Extract sentences that have a single confidence label attached, at the end of the sentence, in parentheses. Split it into sentence/statement and the confidence label(s) as a tuple. \n",
    "\n",
    "regex = '([A-Z]+.*)\\((((very )*low|(very )*high|low|medium) confidence)\\).*\\.'\n",
    "\n",
    "filenames = []\n",
    "page_nums = []\n",
    "sent_nums = []\n",
    "texts = []\n",
    "confidence_ratings = []\n",
    "\n",
    "for filename, document in zip(files, documents):\n",
    "    for page_num, page in enumerate(document['text_by_page_sentences']):\n",
    "        for sent_num, sentence in enumerate(page):\n",
    "            match = re.match(regex, sentence)\n",
    "            if match is not None:\n",
    "                text, _, confidence_rating = match.groups()[:3]\n",
    "                filenames.append(filename)\n",
    "                page_nums.append(page_num)\n",
    "                sent_nums.append(sent_num)\n",
    "                texts.append(text)\n",
    "                confidence_ratings.append(confidence_rating)\n",
    "assert len(texts) == len(confidence_ratings)\n",
    "print(len(confidence_ratings))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in a dataframe and export to CSV.\n",
    "import pandas as pd\n",
    "sentences_with_ratings_df = pd.DataFrame(list(zip(filenames, page_nums, sent_nums, texts, confidence_ratings)), columns = ['filenames', 'page_num', 'sent_num', 'text', 'confidence_rating'])\n",
    "\n",
    "sentences_with_ratings_df.to_csv('data/text_processing/sentences_with_ratings_04_20.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect ALL sentences (for context retrieval) and put into a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5642\n"
     ]
    }
   ],
   "source": [
    "filenames_all = []\n",
    "page_nums_all = []\n",
    "sent_nums_all = []\n",
    "texts_all = []\n",
    "\n",
    "for filename, document in zip(files, documents):\n",
    "    for page_num, page in enumerate(document['text_by_page_sentences']):\n",
    "        for sent_num, sentence in enumerate(page):\n",
    "            filenames_all.append(filename)\n",
    "            page_nums_all.append(page_num)\n",
    "            sent_nums_all.append(sent_num)\n",
    "            texts_all.append(sentence)\n",
    "print(len(texts_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_df = pd.DataFrame(list(zip(filenames_all, page_nums_all, sent_nums_all, texts_all)), columns = ['filenames', 'page_num', 'sent_num', 'text'])\n",
    "\n",
    "all_sentences_df.to_csv('data/text_processing/all_sentences.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-llms-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
